nohup python -u main_base_rl_v1.py > output_main_base_rl_v1.txt &


・疑問点
1．最終層は単語１つなのにTLを利用する意味があるのか．
2．attention-head の数は64次元に合わせればいいか


・強化学習
色んな遷移を試し，最適なパスを探していく強化学習．ここでは方策を学習する．
network としては，以下の3つを結合したものから最終的な遷移確率を求める
1．embedding -> arg labels
2．arg_left_indication
3. prev_trans_argument


選択した遷移（アーギュメント）のスパンを正解できたかどうかで点数をつける．
Loss値は大きいほうがよく，REINFORCEのアルゴリズムで値を更新する．（数式は分析モデルを参照）
学習に際して，様々なパスを探索したいため，フィルターを掛けたり，完全な正解を選択することはしない．
 - 修正：ルールに則すためにフィルターは掛ける．フィルターによって選択された行動がいいか悪いかはリワードによって決められるため問題にならない．
         SRLでもたとえ，a->b->c において，最後のcのラベルが明示的にわかったとしても，start, endが合うかどうかは定かではない．
Gumbel-Softmaxを用いた方策は、探索（新しいアクションの試行）と利用（既知の良いアクションの選択）のバランスを学習することが可能．
学習が上手くいっているかどうかは利用を基に算出する．検証データに対して，gumbel-softmaxではなく，softmaxによる遷移を利用してパスを決定．
ここでの値を基にearly stopを判定する．

述語の選択に関しては，膨大な次元数故にスパースな情報となってしまうため，スコアベースで選択する．
Q学習やSARSAが一般的らしい．（強化学習重要１）

模倣学習中に強化学習を行う？
→ つまり，パスが決まった状態で推論を行うことになり，agentがパスを決めていないので目的が変わってしまうのではないか
→ 学習順序を学習するのであり，SRLモデルを直接改善するわけではないという認識もあり，学習中に学ぶ意味がない．

モデルの解釈（どれだけ読んでも以下のどちらかはわからなかった）
1．模倣学習後，強化学習を行う．学習終了後，強化学習モデルが示す遷移で新しい模倣学習モデルを作成する．
    懸念：学習済みモデルでの強化学習であり，新規のモデルに適用してもよいのか？（token_embを利用しないのであればその限りではない）
2．模倣学習中に強化学習も行う．模倣学習中の遷移パスはサンプリングにより決定する．βをうまく設定して探索と利用の割合を決める．
    懸念：強化学習モデルが最適かわからない状態での遷移を基にしている．
          → ランダムより良ければよいと割り切る？
        ：各モデルは独立で学習させるべきか否か．（token_embを利用しないのであればその限りではない）
          → 計算グラフを分割しない場合と，そうでない場合の2つでモデルを組む．
モデルを3つ組，最適なものを選択．


やること
・ Passport
・ ラベル推測改善とprecision改善策
    - iter位置組み込み
    - labelデコーダの学習率を調整（複雑なので下げる）
    - ラベルミスが大きい
        1. labelデコーダの層を厚く強化する
        2. detach()させる
paddingの仕様
述語の強化学習方法
   - スコア基準でおこなう．
   - datasetの単位を1とし，勾配累積で何とかする